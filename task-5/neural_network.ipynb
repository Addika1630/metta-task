{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  Simple Neural Network from Scratch using NumPy\n",
        "\n",
        "## ðŸ“Œ Project Overview\n",
        "\n",
        "This notebook demonstrates how to build and train a simple neural network **from scratch** using **NumPy**, without relying on high-level deep learning frameworks like TensorFlow or PyTorch.\n",
        "\n",
        "The project follows these key goals:\n",
        "\n",
        "1. **Implement Dense (Fully Connected) Layers**\n",
        "2. **Use ReLU and Softmax Activation Functions**\n",
        "3. **Perform Forward and Backward Propagation**\n",
        "4. **Train with Gradient Descent using Batch Processing**\n",
        "5. **Use Cross-Entropy Loss Function**\n",
        "6. **Evaluate Accuracy on the Wine Dataset**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§ª Dataset\n",
        "\n",
        "We use the **Wine dataset** from the UCI Machine Learning Repository:\n",
        "- 178 samples with 13 numerical features each.\n",
        "- 3 target classes representing different cultivars of wine.\n",
        "- Labels are one-hot encoded for training.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”§ Core Components Implemented\n",
        "\n",
        "- **Activation Functions**: `ReLU`, `Softmax`, and their derivatives\n",
        "- **Loss Function**: `Cross-Entropy` and its derivative\n",
        "- **Dense Layer Class**: Handles weights, bias, forward and backward propagation\n",
        "- **Neural Network Class**: Composed of 3 layers (each with 1 neuron for simplicity)\n",
        "- **Training Loop**: Supports batch training with manual weight updates\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## ðŸ“Ž Note\n",
        "\n",
        "This notebook is structured for clarity and ease of understanding. Each code block is followed by short explanations and outputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "e5g9Fl6UzPJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import urllib.request\n"
      ],
      "metadata": {
        "id": "t6cwWeTDhFzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zSyFPMz-Ote"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import numpy as np\n",
        "\n",
        "# Load the Wine dataset from the UCI Machine Learning Repository\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'\n",
        "raw_data = urllib.request.urlopen(url)\n",
        "data = np.genfromtxt(raw_data, delimiter=',')\n",
        "\n",
        "\"\"\"\n",
        "The dataset has class labels in the first column (1, 2, or 3) and\n",
        "13 numerical features in the remaining columns.\n",
        "We separate them into `y` (labels) and `X` (features).\n",
        "\"\"\"\n",
        "y = data[:, 0].astype(int)\n",
        "X = data[:, 1:]\n",
        "\n",
        "\"\"\"\n",
        "Normalize the features to have zero mean and unit variance.\n",
        "This helps the neural network converge faster during training.\n",
        "\"\"\"\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "\"\"\"\n",
        "Define a one-hot encoding function.\n",
        "For a multiclass classification task, we convert the integer class labels into\n",
        "binary vectors. For example, class 1 becomes [1, 0, 0], class 2 becomes [0, 1, 0], etc.\n",
        "\"\"\"\n",
        "def one_hot_encode(y, num_classes=None):\n",
        "    if num_classes is None:\n",
        "        num_classes = np.max(y)\n",
        "    return np.eye(num_classes)[y - 1]  # subtract 1 to shift classes to 0-based index\n",
        "\n",
        "# Apply one-hot encoding to the labels\n",
        "y_encoded = one_hot_encode(y, num_classes=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Activation and loss functions:\n",
        "- relu: activation function, replaces negatives with 0\n",
        "- relu_derivative: gradient for relu\n",
        "- softmax: normalizes outputs into probabilities\n",
        "- cross_entropy: loss function for classification\n",
        "- cross_entropy_derivative: gradient of loss w.r.t predictions\n",
        "\"\"\"\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy(predictions, targets):\n",
        "    m = targets.shape[0]\n",
        "    return -np.sum(targets * np.log(predictions + 1e-15)) / m\n",
        "\n",
        "def cross_entropy_derivative(predictions, targets):\n",
        "    return predictions - targets\n"
      ],
      "metadata": {
        "id": "RpMFiYfKf0lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer:\n",
        "    def __init__(self, input_size, activation):\n",
        "        self.weights = np.random.randn(input_size, 1) * 0.01\n",
        "        self.bias = np.zeros((1,))\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        self.linear_output = np.dot(input, self.weights) + self.bias\n",
        "\n",
        "        if self.activation == 'relu':\n",
        "            self.output = relu(self.linear_output)\n",
        "        elif self.activation == 'softmax':\n",
        "            self.output = softmax(self.linear_output)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_error, learning_rate):\n",
        "        if self.activation == 'relu':\n",
        "            delta = output_error * relu_derivative(self.linear_output)\n",
        "        elif self.activation == 'softmax':\n",
        "            delta = output_error\n",
        "\n",
        "        input_error = np.dot(delta, self.weights.T)\n",
        "        weights_error = np.dot(self.input.T, delta)\n",
        "\n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.bias -= learning_rate * np.sum(delta, axis=0)\n",
        "\n",
        "        return input_error"
      ],
      "metadata": {
        "id": "NQ7A0oDEf0rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNeuralNetwork:\n",
        "    \"\"\"\n",
        "    DenseLayer:\n",
        "    - Initializes weights and biases for a single neuron layer.\n",
        "    - Supports ReLU and Softmax activation.\n",
        "    - Performs forward pass (linear + activation).\n",
        "    - Performs backward pass (computes gradients and updates weights).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.layer1 = DenseLayer(input_dim, 'relu')\n",
        "        self.layer2 = DenseLayer(1, 'relu')\n",
        "        self.layer3 = DenseLayer(1, 'softmax')  # softmax later handled manually\n",
        "\n",
        "        # Final transformation layer\n",
        "        self.output_weights = np.random.randn(1, output_dim) * 0.01\n",
        "        self.output_bias = np.zeros((1, output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.layer1.forward(x)\n",
        "        out2 = self.layer2.forward(out1)\n",
        "        out3 = self.layer3.forward(out2)\n",
        "        self.logits = np.dot(out3, self.output_weights) + self.output_bias\n",
        "        self.output = softmax(self.logits)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, y_true, learning_rate):\n",
        "        error = cross_entropy_derivative(self.output, y_true)\n",
        "        d_logits = error\n",
        "\n",
        "        d_output_weights = np.dot(self.layer3.output.T, d_logits)\n",
        "        d_output_bias = np.sum(d_logits, axis=0, keepdims=True)\n",
        "        d_hidden = np.dot(d_logits, self.output_weights.T)\n",
        "\n",
        "        self.output_weights -= learning_rate * d_output_weights\n",
        "        self.output_bias -= learning_rate * d_output_bias\n",
        "\n",
        "        d_out3 = self.layer3.backward(d_hidden, learning_rate)\n",
        "        d_out2 = self.layer2.backward(d_out3, learning_rate)\n",
        "        self.layer1.backward(d_out2, learning_rate)\n",
        "\n",
        "    def train(self, X, y, epochs=1000, batch_size=16, learning_rate=0.1):\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                X_batch = X_shuffled[i:i+batch_size]\n",
        "                y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "                output = self.forward(X_batch)\n",
        "                loss = cross_entropy(output, y_batch)\n",
        "                self.backward(y_batch, learning_rate)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        return np.argmax(output, axis=1)\n"
      ],
      "metadata": {
        "id": "VY2rVtoTga1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Initialize and train the neural network:\n",
        "- Creates a model with input size equal to feature dimension and 3 output classes.\n",
        "- Trains the model using cross-entropy loss and batch gradient descent.\n",
        "\"\"\"\n",
        "\n",
        "model = SimpleNeuralNetwork(input_dim=X.shape[1], output_dim=3)\n",
        "\n",
        "# Train\n",
        "model.train(X, y_encoded, epochs=1000, batch_size=16, learning_rate=0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7WFlJX6f1Ad",
        "outputId": "da2ec59a-5700-4c2e-e30e-4768efeaeaef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.0829\n",
            "Epoch 100, Loss: 1.1067\n",
            "Epoch 200, Loss: 1.1067\n",
            "Epoch 300, Loss: 1.1047\n",
            "Epoch 400, Loss: 1.2083\n",
            "Epoch 500, Loss: 1.1051\n",
            "Epoch 600, Loss: 1.0119\n",
            "Epoch 700, Loss: 1.2082\n",
            "Epoch 800, Loss: 1.0121\n",
            "Epoch 900, Loss: 1.3132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Evaluate model performance:\n",
        "- Predict class labels on training data.\n",
        "- Compare with true labels to calculate accuracy.\n",
        "\"\"\"\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "y_true = np.argmax(y_encoded, axis=1)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = np.mean(y_pred == y_true)\n",
        "print(f\"\\nFinal Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1Xn66EPih8D",
        "outputId": "f155eaae-75e8-4179-a0dc-4a029360563d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Accuracy: 39.89%\n"
          ]
        }
      ]
    }
  ]
}